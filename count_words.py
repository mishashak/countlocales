import os
import sys
import pandas as pd
import re
from openpyxl import Workbook
from openpyxl.utils import get_column_letter
from openpyxl.styles import Alignment
from datetime import datetime
from tqdm import tqdm
import tempfile
import json
import shutil
from collections import defaultdict, Counter

# ì–¸ì–´ ê°ì§€ ë° ìì—°ì–´ ì²˜ë¦¬ ë¼ì´ë¸ŒëŸ¬ë¦¬
try:
    from langdetect import detect, DetectorFactory
    DetectorFactory.seed = 0  # ì¬í˜„ ê°€ëŠ¥í•œ ê²°ê³¼ë¥¼ ìœ„í•´
except ImportError:
    print("Warning: langdetect not installed. Please install with: pip install langdetect")
    detect = None

try:
    from kiwipiepy import Kiwi
    kiwi = Kiwi()
    print("Korean processor: Kiwi loaded successfully")
except (ImportError, Exception) as e:
    print(f"Warning: kiwipiepy not available ({e}). Korean text will use basic split().")
    kiwi = None

try:
    import spacy
    # ë‹¤ì–‘í•œ ì–¸ì–´ ëª¨ë¸ ë¡œë“œ
    nlp_models = {}
    
    # ì§€ì›í•˜ëŠ” spacy ëª¨ë¸ë“¤ (í˜„ì¬ ë²„ì „ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ê²ƒë“¤ë§Œ)
    spacy_models = {
        'en': 'en_core_web_sm',
        'es': 'es_core_news_sm', 
        'fr': 'fr_core_news_sm',
        'de': 'de_core_news_sm',
        'pt': 'pt_core_news_sm',
        'it': 'it_core_news_sm',
        'ru': 'ru_core_news_sm'
        # tr, vi, th, id ëª¨ë¸ë“¤ì€ í˜„ì¬ spaCy ë²„ì „ì—ì„œ ì§€ì›ë˜ì§€ ì•ŠìŒ
    }
    
    for lang_code, model_name in spacy_models.items():
        try:
            # PyInstaller í™˜ê²½ì—ì„œ ëª¨ë¸ ê²½ë¡œ ì°¾ê¸°
            import sys
            import os
            
            if getattr(sys, 'frozen', False):
                # ì‹¤í–‰ íŒŒì¼ í™˜ê²½
                base_path = sys._MEIPASS
                model_path = os.path.join(base_path, 'spacy_models', model_name)
                
                # ëª¨ë¸ ê²½ë¡œ í™•ì¸ ë° ë¡œë“œ
                if os.path.exists(model_path):
                    # config.cfg íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸
                    config_path = os.path.join(model_path, f'{model_name}-3.8.0', 'config.cfg')
                    if os.path.exists(config_path):
                        nlp_models[lang_code] = spacy.load(model_path)
                        print(f"Loaded {lang_code} model from bundled path: {model_path}")
                    else:
                        # ì „ì²´ ëª¨ë¸ ë””ë ‰í† ë¦¬ì—ì„œ ì°¾ê¸°
                        for root, dirs, files in os.walk(model_path):
                            if 'config.cfg' in files:
                                actual_model_path = root
                                nlp_models[lang_code] = spacy.load(actual_model_path)
                                print(f"Loaded {lang_code} model from: {actual_model_path}")
                                break
                        else:
                            raise Exception(f"config.cfg not found in {model_path}")
                else:
                    # ê¸°ë³¸ ê²½ë¡œë¡œ ì‹œë„
                    nlp_models[lang_code] = spacy.load(model_name)
                    print(f"Loaded {lang_code} model: {model_name}")
            else:
                # ê°œë°œ í™˜ê²½
                nlp_models[lang_code] = spacy.load(model_name)
                print(f"Loaded {lang_code} model: {model_name}")
        except Exception as e:
            print(f"Warning: {model_name} not available ({e}). Will use basic split().")
        
except ImportError:
    print("Warning: spacy not installed. Please install with: pip install spacy")
    nlp_models = {}

try:
    import jieba
except ImportError:
    print("Warning: jieba not installed. Please install with: pip install jieba")
    jieba = None

try:
    import stanza
    # ì¼ë³¸ì–´ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° ë¡œë“œ
    try:
        nlp_ja = stanza.Pipeline('ja', verbose=False)
        print("Japanese processor: Stanza loaded successfully")
    except:
        # ëª¨ë¸ì´ ì—†ìœ¼ë©´ ë‹¤ìš´ë¡œë“œ ì‹œë„
        stanza.download('ja', verbose=False)
        nlp_ja = stanza.Pipeline('ja', verbose=False)
        print("Japanese processor: Stanza loaded successfully")
except (ImportError, Exception) as e:
    print(f"Warning: stanza not available ({e}). Japanese text will use basic split().")
    nlp_ja = None

from translations import t

# ì§€ì› ì–¸ì–´ ë§¤í•‘ (langdetect ì½”ë“œ -> í‘œì‹œëª…)
LANGUAGE_MAPPING = {
    'ko': 'Korean',
    'en': 'English', 
    'zh-cn': 'Simplified_Chinese',
    'zh-tw': 'Traditional_Chinese',
    'ja': 'Japanese',
    'vi': 'Vietnamese',
    'th': 'Thai',
    'id': 'Indonesian',
    'ru': 'Russian',
    'es': 'Spanish',
    'pt': 'Portuguese',
    'tr': 'Turkish',
    'fr': 'French',
    'it': 'Italian',
    'de': 'German'
}

# HTML/XML ë° íŠ¹ìˆ˜ í…ìŠ¤íŠ¸ íŒ¨í„´
SPECIAL_PATTERNS = {
    'html_xml': re.compile(r'(</?[^<>]*?>)'),
    'brackets': re.compile(r'(\{[^{}]+\})'),
    'newlines': re.compile(r'(\\n)'),
    'file_paths': re.compile(r'([a-zA-Z]:\\[^ ]+|/[^ ]+)')
}

def detect_language(text):
    """í…ìŠ¤íŠ¸ì˜ ì–¸ì–´ë¥¼ ê°ì§€"""
    if not detect:
        return 'unknown'
    
    try:
        # ë„ˆë¬´ ì§§ì€ í…ìŠ¤íŠ¸ëŠ” ê°ì§€í•˜ì§€ ì•ŠìŒ
        if len(text.strip()) < 3:
            return 'unknown'
        return detect(text)
    except:
        return 'unknown'

def process_text_by_language(text, language):
    """ì–¸ì–´ë³„ë¡œ í…ìŠ¤íŠ¸ë¥¼ ë‹¨ì–´ë¡œ ë¶„ë¦¬"""
    if not text or pd.isna(text) or str(text).strip() == '':
        return []
    
    text = str(text).strip()
    
    # íŠ¹ìˆ˜ íŒ¨í„´ ì œê±°
    clean_text = text
    for pattern_name, pattern in SPECIAL_PATTERNS.items():
        clean_text = pattern.sub(' ', clean_text)
    
    # ì „ì²˜ë¦¬: êµ¬ë‘ì ê³¼ í•˜ì´í”ˆ ì œê±°, ìˆ«ì/ë‚ ì§œ/ë²„ì „ íŒ¨í„´ ë³´ì¡´
    clean_text = preprocess_text(clean_text)
    
    # ê³µë°±ìœ¼ë¡œ ë¶„ë¦¬í•˜ì—¬ ê¸°ë³¸ ë‹¨ì–´ ì¶”ì¶œ
    words = []
    
    if language == 'ko' and kiwi:
        # í•œêµ­ì–´: Kiwi ì‚¬ìš©
        try:
            tokens = kiwi.tokenize(clean_text)
            words = [token.form for token in tokens if token.form.strip()]
        except:
            words = clean_text.split()
    
    elif language in nlp_models:
        # spaCy ì§€ì› ì–¸ì–´ë“¤: ì˜ì–´, ìŠ¤í˜ì¸ì–´, í”„ë‘ìŠ¤ì–´, ë…ì¼ì–´, í¬ë¥´íˆ¬ê°ˆì–´, ì´íƒˆë¦¬ì•„ì–´, ëŸ¬ì‹œì•„ì–´, í„°í‚¤ì–´, ë² íŠ¸ë‚¨ì–´, íƒœêµ­ì–´, ì¸ë„ë„¤ì‹œì•„ì–´
        try:
            doc = nlp_models[language](clean_text)
            words = [token.text for token in doc if not token.is_space and token.text.strip()]
        except:
            words = clean_text.split()
    
    elif language in ['zh-cn', 'zh-tw'] and jieba:
        # ì¤‘êµ­ì–´(ê°„ì²´/ë²ˆì²´): jieba ì‚¬ìš©
        try:
            words = list(jieba.cut(clean_text))
            words = [word for word in words if word.strip()]
        except:
            words = clean_text.split()
    
    elif language == 'ja' and nlp_ja:
        # ì¼ë³¸ì–´: Stanza ì‚¬ìš©
        try:
            doc = nlp_ja(clean_text)
            words = []
            for sent in doc.sentences:
                for token in sent.tokens:
                    words.append(token.text)
        except:
            words = clean_text.split()
    
    else:
        # ê¸°íƒ€ ì–¸ì–´: ê¸°ë³¸ split() ì‚¬ìš©
        words = clean_text.split()
    
    return [word for word in words if word.strip()]

def extract_special_patterns(text):
    """íŠ¹ìˆ˜ íŒ¨í„´ë“¤ì„ ì¶”ì¶œí•˜ì—¬ ì¹´í…Œê³ ë¦¬ë³„ë¡œ ë¶„ë¥˜"""
    if not text or pd.isna(text) or str(text).strip() == '':
        return {}
    
    text = str(text)
    pattern_counts = {}
    
    for pattern_name, pattern in SPECIAL_PATTERNS.items():
        matches = pattern.findall(text)
        pattern_counts[pattern_name] = len(matches)
    
    return pattern_counts

def preprocess_text(text):
    """í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬: êµ¬ë‘ì /í•˜ì´í”ˆ ì œê±°, ìˆ«ì/ë‚ ì§œ/ë²„ì „ íŒ¨í„´ ë³´ì¡´"""
    import re
    
    # ìˆ«ì, ë‚ ì§œ, ë²„ì „ íŒ¨í„´ë“¤ì„ ë¨¼ì € ë³´í˜¸ (ì„ì‹œ í”Œë ˆì´ìŠ¤í™€ë”ë¡œ êµì²´)
    protected_patterns = []
    
    # ë²„ì „ íŒ¨í„´ (ì˜ˆ: 1.0.4, 2.1.3.5, v1.2.3)
    version_pattern = r'\b(?:v)?\d+(?:\.\d+){1,3}\b'
    for i, match in enumerate(re.finditer(version_pattern, text)):
        placeholder = f"__VERSION_{i}__"
        protected_patterns.append((placeholder, match.group()))
        text = text.replace(match.group(), placeholder, 1)
    
    # ë‚ ì§œ íŒ¨í„´ (ì˜ˆ: 2024-01-15, 15/01/2024, 2024.01.15)
    date_patterns = [
        r'\b\d{4}[-/.]\d{1,2}[-/.]\d{1,2}\b',  # YYYY-MM-DD, YYYY/MM/DD, YYYY.MM.DD
        r'\b\d{1,2}[-/.]\d{1,2}[-/.]\d{4}\b',  # MM-DD-YYYY, MM/DD/YYYY, MM.DD.YYYY
        r'\b\d{1,2}[-/.]\d{1,2}[-/.]\d{2}\b'   # MM-DD-YY, MM/DD/YY, MM.DD.YY
    ]
    for pattern in date_patterns:
        for i, match in enumerate(re.finditer(pattern, text)):
            placeholder = f"__DATE_{len(protected_patterns)}__"
            protected_patterns.append((placeholder, match.group()))
            text = text.replace(match.group(), placeholder, 1)
    
    # ì‹œê°„ íŒ¨í„´ (ì˜ˆ: 14:30, 2:30:45)
    time_pattern = r'\b\d{1,2}:\d{2}(?::\d{2})?\b'
    for i, match in enumerate(re.finditer(time_pattern, text)):
        placeholder = f"__TIME_{len(protected_patterns)}__"
        protected_patterns.append((placeholder, match.group()))
        text = text.replace(match.group(), placeholder, 1)
    
    # ìˆ«ì íŒ¨í„´ (ì •ìˆ˜, ì†Œìˆ˜, í¼ì„¼íŠ¸, í†µí™”)
    number_patterns = [
        r'\b\d+\.\d+\b',      # ì†Œìˆ˜ (ì˜ˆ: 3.14, 123.45)
        r'\b\d+%\b',          # í¼ì„¼íŠ¸ (ì˜ˆ: 50%, 100%)
        r'\b\d+[km]?\b',      # ìˆ«ì + ë‹¨ìœ„ (ì˜ˆ: 100, 5k, 2m)
        r'\$\d+(?:\.\d{2})?\b',  # í†µí™” (ì˜ˆ: $100, $99.99)
        r'\b\d+\b'            # ì •ìˆ˜ (ì˜ˆ: 123, 456)
    ]
    for pattern in number_patterns:
        for i, match in enumerate(re.finditer(pattern, text)):
            placeholder = f"__NUMBER_{len(protected_patterns)}__"
            protected_patterns.append((placeholder, match.group()))
            text = text.replace(match.group(), placeholder, 1)
    
    # í•˜ì´í”ˆ ì œê±° (ë‹¨ì–´ í•©ì¹˜ê¸°)
    text = re.sub(r'-', '', text)
    
    # êµ¬ë‘ì  ì œê±° (ê³µë°±ìœ¼ë¡œ ëŒ€ì²´)  
    text = re.sub(r'[^\w\s]', ' ', text)
    
    # ì—°ì†ëœ ê³µë°±ì„ í•˜ë‚˜ë¡œ ì •ë¦¬
    text = re.sub(r'\s+', ' ', text).strip()
    
    # ë³´í˜¸ëœ íŒ¨í„´ë“¤ì„ ì›ë˜ ê°’ìœ¼ë¡œ ë³µì›
    for placeholder, original in protected_patterns:
        text = text.replace(placeholder, original)
    
    return text

def detect_column_language(df, column_index):
    """íŠ¹ì • ì—´ì˜ ëª¨ë“  ì…€ì„ ë¶„ì„í•˜ì—¬ ê°€ì¥ ë§ì´ ë‚˜íƒ€ë‚˜ëŠ” ì–¸ì–´ë¥¼ ë°˜í™˜"""
    if not detect:
        return 'unknown'
    
    language_votes = []
    
    for row in range(df.shape[0]):
        cell_value = df.iat[row, column_index]
        if pd.isna(cell_value) or str(cell_value).strip() == '':
            continue
        
        text = str(cell_value)
        if len(text.strip()) >= 3:  # ìµœì†Œ ê¸¸ì´ ì²´í¬
            detected_lang = detect_language(text)
            if detected_lang != 'unknown':
                language_votes.append(detected_lang)
    
    if not language_votes:
        return 'unknown'
    
    # ê°€ì¥ ë§ì´ ë‚˜íƒ€ë‚˜ëŠ” ì–¸ì–´ ë°˜í™˜
    return Counter(language_votes).most_common(1)[0][0]

def count_words_in_text(text, language):
    """í…ìŠ¤íŠ¸ì—ì„œ ë‹¨ì–´ ìˆ˜ë¥¼ ê³„ì‚° (ì¤‘ë³µ í¬í•¨)"""
    words = process_text_by_language(text, language)
    return len(words)

def count_unique_words_in_text(text, language):
    """í…ìŠ¤íŠ¸ì—ì„œ ê³ ìœ  ë‹¨ì–´ ìˆ˜ë¥¼ ê³„ì‚°"""
    words = process_text_by_language(text, language)
    return len(set(words))

# ì„ì‹œ íŒŒì¼ ê´€ë¦¬ë¥¼ ìœ„í•œ í´ë˜ìŠ¤ (ë‹¨ì–´ìš©)
class TempWordManager:
    def __init__(self, base_dir):
        self.base_dir = base_dir
        self.temp_dir = tempfile.mkdtemp(dir=base_dir)
        self.temp_files = defaultdict(list)
        self.current_sets = defaultdict(set)
        self.set_size_limit = 100000

    def add_words(self, category, words):
        for word in words:
            if word not in self.current_sets[category]:
                self.current_sets[category].add(word)
                if len(self.current_sets[category]) >= self.set_size_limit:
                    self._save_to_temp_file(category)

    def _save_to_temp_file(self, category):
        if not self.current_sets[category]:
            return

        temp_file = os.path.join(self.temp_dir, f"{category}_{len(self.temp_files[category])}.json")
        with open(temp_file, 'w', encoding='utf-8') as f:
            json.dump(list(self.current_sets[category]), f, ensure_ascii=False)
        
        self.temp_files[category].append(temp_file)
        self.current_sets[category].clear()

    def get_all_unique_words(self, category):
        all_words = set()
        
        # í˜„ì¬ ë©”ëª¨ë¦¬ì— ìˆëŠ” ë‹¨ì–´ ì¶”ê°€
        all_words.update(self.current_sets[category])
        
        # ì„ì‹œ íŒŒì¼ì—ì„œ ë‹¨ì–´ ë¡œë“œ
        for temp_file in self.temp_files[category]:
            with open(temp_file, 'r', encoding='utf-8') as f:
                all_words.update(json.load(f))
        
        return all_words

    def cleanup(self):
        shutil.rmtree(self.temp_dir)

def analyze_sheet_for_words(df):
    """ì‹œíŠ¸ë¥¼ ë¶„ì„í•˜ì—¬ ë‹¨ì–´ ìˆ˜ë¥¼ ê³„ì‚°"""
    # ë¨¼ì € ê° ì—´ì˜ ì–¸ì–´ë¥¼ ê°ì§€
    column_languages = {}
    for col in range(df.shape[1]):
        column_languages[col] = detect_column_language(df, col)
    
    # ì „ì²´ ì¹´í…Œê³ ë¦¬ (ì–¸ì–´ + íŠ¹ìˆ˜ íŒ¨í„´)
    all_categories = set()
    for lang_code in column_languages.values():
        if lang_code != 'unknown':
            # langdetect ì½”ë“œë¥¼ í‘œì‹œëª…ìœ¼ë¡œ ë³€í™˜
            display_name = LANGUAGE_MAPPING.get(lang_code, lang_code)
            all_categories.add(display_name)
    
    all_categories.update(['html_xml', 'brackets', 'newlines', 'file_paths'])
    
    total_counts = {category: 0 for category in all_categories}
    column_counts = {col: {category: 0 for category in all_categories} for col in range(df.shape[1])}
    
    for r in range(df.shape[0]):
        for c in range(df.shape[1]):
            cell_value = df.iat[r, c]
            if pd.isna(cell_value) or str(cell_value).strip() == '':
                continue
            
            text = str(cell_value)
            col_lang = column_languages[c]
            
            if col_lang != 'unknown':
                # langdetect ì½”ë“œë¥¼ í‘œì‹œëª…ìœ¼ë¡œ ë³€í™˜
                display_name = LANGUAGE_MAPPING.get(col_lang, col_lang)
                # ë‹¨ì–´ ìˆ˜ ê³„ì‚°
                word_count = count_words_in_text(text, col_lang)
                total_counts[display_name] += word_count
                column_counts[c][display_name] += word_count
            
            # íŠ¹ìˆ˜ íŒ¨í„´ ì¹´ìš´íŠ¸
            special_patterns = extract_special_patterns(text)
            for pattern_name, count in special_patterns.items():
                if count > 0:
                    total_counts[pattern_name] += count
                    column_counts[c][pattern_name] += count
    
    # ìœ íš¨í•œ ì—´ë§Œ í•„í„°ë§
    valid_columns = []
    empty_col_count = 0
    for col in range(df.shape[1]):
        col_total = sum(column_counts[col].values())
        if col_total > 0:
            valid_columns.append(col)
            empty_col_count = 0
        else:
            empty_col_count += 1
            if empty_col_count >= 20:
                break
    
    return total_counts, column_counts, valid_columns, column_languages

def get_unique_words_per_column(df, column_languages):
    """ê° ì—´ë³„ë¡œ ê³ ìœ  ë‹¨ì–´ ìˆ˜ë¥¼ ê³„ì‚°"""
    unique_counts = {}
    for col in range(df.shape[1]):
        col_lang = column_languages.get(col, 'unknown')
        
        # ì „ì²´ ì¹´í…Œê³ ë¦¬ ìƒì„±
        all_categories = set()
        for lang_code in column_languages.values():
            if lang_code != 'unknown':
                display_name = LANGUAGE_MAPPING.get(lang_code, lang_code)
                all_categories.add(display_name)
        all_categories.update(['html_xml', 'brackets', 'newlines', 'file_paths'])
        
        col_counts = {category: 0 for category in all_categories}
        
        # í•´ë‹¹ ì—´ì˜ ëª¨ë“  ê°’ì„ ê°€ì ¸ì˜´
        column_values = df.iloc[:, col].dropna().astype(str)
        unique_texts = column_values.unique()
        
        # ê° ê³ ìœ  ê°’ì— ëŒ€í•´ ë‹¨ì–´ ìˆ˜ë¥¼ ì„¸ê³  í•©ì‚°
        for value in unique_texts:
            if col_lang != 'unknown':
                display_name = LANGUAGE_MAPPING.get(col_lang, col_lang)
                unique_word_count = count_unique_words_in_text(value, col_lang)
                col_counts[display_name] += unique_word_count
            
            # íŠ¹ìˆ˜ íŒ¨í„´ ì¹´ìš´íŠ¸
            special_patterns = extract_special_patterns(value)
            for pattern_name, count in special_patterns.items():
                col_counts[pattern_name] += count
        
        unique_counts[col] = col_counts
    
    return unique_counts

def get_cell_addresses_for_words(df, column_languages):
    """ë‹¨ì–´ ìˆ˜ ë¶„ì„ìš© ì…€ ì£¼ì†Œ ì¶”ì¶œ"""
    cell_addresses = {}
    
    # ì „ì²´ ì¹´í…Œê³ ë¦¬ ìƒì„±
    all_categories = set()
    for lang_code in column_languages.values():
        if lang_code != 'unknown':
            display_name = LANGUAGE_MAPPING.get(lang_code, lang_code)
            all_categories.add(display_name)
    all_categories.update(['html_xml', 'brackets', 'newlines', 'file_paths'])
    
    # ê° ì¹´í…Œê³ ë¦¬ë³„ë¡œ ì…€ ì£¼ì†Œ ë”•ì…”ë„ˆë¦¬ ì´ˆê¸°í™”
    for category in all_categories:
        cell_addresses[category] = {col: [] for col in range(df.shape[1])}
    
    for r in range(df.shape[0]):
        for c in range(df.shape[1]):
            cell_value = df.iat[r, c]
            if pd.isna(cell_value) or str(cell_value).strip() == '':
                continue
                
            text = str(cell_value)
            col_lang = column_languages.get(c, 'unknown')
            
            if col_lang != 'unknown':
                # langdetect ì½”ë“œë¥¼ í‘œì‹œëª…ìœ¼ë¡œ ë³€í™˜
                display_name = LANGUAGE_MAPPING.get(col_lang, col_lang)
                # ë‹¨ì–´ê°€ ìˆëŠ”ì§€ í™•ì¸
                word_count = count_words_in_text(text, col_lang)
                if word_count > 0:
                    cell_address = f"{get_column_letter(c+1)}{r+1}"
                    cell_addresses[display_name][c].append(cell_address)
            
            # íŠ¹ìˆ˜ íŒ¨í„´ í™•ì¸
            special_patterns = extract_special_patterns(text)
            for pattern_name, count in special_patterns.items():
                if count > 0:
                    cell_address = f"{get_column_letter(c+1)}{r+1}"
                    cell_addresses[pattern_name][c].append(cell_address)
    
    # ê° ì¹´í…Œê³ ë¦¬ë³„ë¡œ ì…€ ì£¼ì†Œ ì •ë ¬
    for category in all_categories:
        for col in range(df.shape[1]):
            cell_addresses[category][col].sort(key=lambda x: (x[0], int(x[1:])))
    
    return cell_addresses

def count_cells_by_category_for_words(df, column_languages):
    """ë‹¨ì–´ ìˆ˜ ë¶„ì„ìš© ì¹´í…Œê³ ë¦¬ë³„ ì…€ ê°œìˆ˜ ê³„ì‚°"""
    cell_counts = {}
    
    # ì „ì²´ ì¹´í…Œê³ ë¦¬ ìƒì„±
    all_categories = set()
    for lang_code in column_languages.values():
        if lang_code != 'unknown':
            display_name = LANGUAGE_MAPPING.get(lang_code, lang_code)
            all_categories.add(display_name)
    all_categories.update(['html_xml', 'brackets', 'newlines', 'file_paths'])
    
    # ê° ì¹´í…Œê³ ë¦¬ë³„ë¡œ ì…€ ê°œìˆ˜ ë”•ì…”ë„ˆë¦¬ ì´ˆê¸°í™”
    for category in all_categories:
        cell_counts[category] = {col: 0 for col in range(df.shape[1])}
    
    for r in range(df.shape[0]):
        for c in range(df.shape[1]):
            cell_value = df.iat[r, c]
            if pd.isna(cell_value) or str(cell_value).strip() == '':
                continue
                
            text = str(cell_value)
            col_lang = column_languages.get(c, 'unknown')
            
            if col_lang != 'unknown':
                # langdetect ì½”ë“œë¥¼ í‘œì‹œëª…ìœ¼ë¡œ ë³€í™˜
                display_name = LANGUAGE_MAPPING.get(col_lang, col_lang)
                # ë‹¨ì–´ê°€ ìˆëŠ”ì§€ í™•ì¸
                word_count = count_words_in_text(text, col_lang)
                if word_count > 0:
                    cell_counts[display_name][c] += 1
            
            # íŠ¹ìˆ˜ íŒ¨í„´ í™•ì¸
            special_patterns = extract_special_patterns(text)
            for pattern_name, count in special_patterns.items():
                if count > 0:
                    cell_counts[pattern_name][c] += 1
    
    return cell_counts

def adjust_column_widths(sheet):
    """ì—´ ë„ˆë¹„ ì¡°ì •"""
    for column_cells in sheet.columns:
        max_length = 0
        column = column_cells[0].column_letter
        
        # Words_cell_address ì‹œíŠ¸ì—ì„œ ë°ì´í„° ì—´ë§Œ ë„ˆë¹„ë¥¼ 10ìœ¼ë¡œ ê³ ì •
        if sheet.title == 'Words_cell_address' and column >= 'G':
            sheet.column_dimensions[column].width = 10
            for cell in column_cells:
                cell.alignment = Alignment(wrap_text=False, shrink_to_fit=True)
            continue
            
        for cell in column_cells:
            try:
                if len(str(cell.value)) > max_length:
                    max_length = len(str(cell.value))
            except:
                pass
        adjusted_width = (max_length + 2)
        sheet.column_dimensions[column].width = adjusted_width

def main(current_language='ko'):
    """ë©”ì¸ í•¨ìˆ˜"""
    # exe íŒŒì¼ì´ ì‹¤í–‰ëœ ê²½ë¡œë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì„¤ì •
    folder_path = os.path.dirname(os.path.abspath(sys.executable)) if getattr(sys, 'frozen', False) else os.path.dirname(os.path.abspath(__file__))
    print(f"{t('UI_006', current_language)}: {folder_path}")
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    report_name = f"WORD_COUNT_REPORT_{timestamp}.xlsx"
    report_path = os.path.join(folder_path, report_name)
    print(f"{t('UI_007', current_language)}: {report_path}")

    report_wb = Workbook()
    
    # 6ê°œì˜ ì‹œíŠ¸ ìƒì„± (count_charsì™€ ë™ì¼í•œ êµ¬ì¡°)
    report_ws_real = report_wb.active
    report_ws_real.title = 'Words_real'
    
    report_ws_unique_for_sheet = report_wb.create_sheet('Words_unique_for_Sheet')
    report_ws_unique_for_folder = report_wb.create_sheet('Words_unique_for_Folder')
    report_ws_cell_address = report_wb.create_sheet('Words_cell_address')
    report_ws_cells = report_wb.create_sheet('Words_cells')
    
    # ì„ì‹œ íŒŒì¼ ë§¤ë‹ˆì € ì´ˆê¸°í™”
    temp_manager = TempWordManager(folder_path)

    # í•˜ìœ„ í´ë”ë¥¼ í¬í•¨í•œ ëª¨ë“  ì—‘ì…€ íŒŒì¼ ìˆ˜ì§‘
    files_to_process = []
    for root, dirs, files in os.walk(folder_path):
        if '__pycache__' in dirs:
            dirs.remove('__pycache__')
        if '.git' in dirs:
            dirs.remove('.git')
            
        for file in files:
            if file.endswith(('.xlsx', '.xlsm', '.csv')) and "REPORT_" not in file and not file.startswith('~$'):
                rel_path = os.path.relpath(os.path.join(root, file), folder_path)
                files_to_process.append((rel_path, file))

    print(f"{t('UI_008', current_language)}: {[f[1] for f in files_to_process]}")
    print(t('UI_009', current_language).format(len(files_to_process)))
    print(t('UI_010', current_language))

    # ì „ì²´ ì—´ì„ ì¶”ì í•˜ê¸° ìœ„í•œ ë³€ìˆ˜
    all_columns = set()
    all_categories = set()
    processed_files = 0

    data_rows_real = []
    data_rows_unique_for_sheet = []
    data_rows_unique_for_folder = []
    data_rows_cell_address = []
    data_rows_cells = []

    # ì½˜ì†” ì¶œë ¥ì´ ê°€ëŠ¥í•œì§€ í™•ì¸
    has_console = hasattr(sys.stdout, 'write') and sys.stdout is not None
    
    for rel_path, file_name in tqdm(files_to_process, desc="processing files", disable=not has_console):
        try:
            print(f"\n{t('UI_011', current_language)}: {file_name}")
            file_path = os.path.join(folder_path, rel_path)
            xls = pd.ExcelFile(file_path)

            for sheet_name in xls.sheet_names:
                print(f"{t('UI_012', current_language)}: {sheet_name}")
                df = pd.read_excel(xls, sheet_name=sheet_name, header=None)
                
                total_counts, column_counts, valid_columns, column_languages = analyze_sheet_for_words(df)
                unique_counts = get_unique_words_per_column(df, column_languages)
                cell_addresses = get_cell_addresses_for_words(df, column_languages)
                cell_counts = count_cells_by_category_for_words(df, column_languages)

                # ìœ íš¨í•œ ì—´ì„ ì „ì²´ ì—´ ëª©ë¡ì— ì¶”ê°€
                for col in valid_columns:
                    all_columns.add(col)

                # ì „ì²´ ì¹´í…Œê³ ë¦¬ ì—…ë°ì´íŠ¸
                all_categories.update(total_counts.keys())

                # ê³ ìœ í•œ ë‹¨ì–´ ìˆ˜ì§‘ (í´ë” ì „ì²´ ê¸°ì¤€)
                for r in range(df.shape[0]):
                    for c in range(df.shape[1]):
                        cell_value = df.iat[r, c]
                        if pd.isna(cell_value) or str(cell_value).strip() == '':
                            continue
                        
                        text = str(cell_value)
                        col_lang = column_languages.get(c, 'unknown')
                        
                        if col_lang != 'unknown':
                            display_name = LANGUAGE_MAPPING.get(col_lang, col_lang)
                            words = process_text_by_language(text, col_lang)
                            temp_manager.add_words(display_name, words)

                # ì‹¤ì œ ë°ì´í„° ì²˜ë¦¬
                for category in total_counts:
                    if category in ['html_xml', 'brackets', 'newlines', 'file_paths']:
                        emoji = 'ğŸ”§'  # íŠ¹ìˆ˜ íŒ¨í„´ìš© ì´ëª¨ì§€
                    else:
                        emoji = 'ğŸŒ'  # ì–¸ì–´ìš© ì´ëª¨ì§€
                    
                    col_totals = [column_counts[col].get(category, 0) for col in valid_columns]
                    total = total_counts[category]
                    sum_col_totals = sum(col_totals)
                    
                    if sum_col_totals != total:
                        status = f"Error: Total words({total}) and column totals({sum_col_totals}) do not match"
                    else:
                        status = "Normal"

                    row_data = [rel_path, file_name, sheet_name, status, emoji, category, total] + col_totals
                    data_rows_real.append(row_data)

                # ê³ ìœ  ê°’ ë°ì´í„° ì²˜ë¦¬
                for category in total_counts:
                    if category in ['html_xml', 'brackets', 'newlines', 'file_paths']:
                        emoji = 'ğŸ”§'
                    else:
                        emoji = 'ğŸŒ'
                    
                    unique_col_totals = [unique_counts[col].get(category, 0) for col in valid_columns]
                    total_unique = sum(unique_col_totals)
                    row_data = [rel_path, file_name, sheet_name, "Normal", emoji, category, total_unique] + unique_col_totals
                    data_rows_unique_for_sheet.append(row_data)
                
                # ì…€ ì£¼ì†Œ ë°ì´í„° ì²˜ë¦¬
                for category in total_counts:
                    if category in ['html_xml', 'brackets', 'newlines', 'file_paths']:
                        emoji = 'ğŸ”§'
                    else:
                        emoji = 'ğŸŒ'
                    
                    cell_col_addresses = [', '.join(cell_addresses[category][col]) for col in valid_columns]
                    total_cells = sum(len(cell_addresses[category][col]) for col in valid_columns)
                    row_data = [rel_path, file_name, sheet_name, "Normal", emoji, category, total_cells] + cell_col_addresses
                    data_rows_cell_address.append(row_data)
                
                # ì…€ ê°¯ìˆ˜ ë°ì´í„° ì²˜ë¦¬
                for category in total_counts:
                    if category in ['html_xml', 'brackets', 'newlines', 'file_paths']:
                        emoji = 'ğŸ”§'
                    else:
                        emoji = 'ğŸŒ'
                    
                    cell_col_counts = [cell_counts[category][col] for col in valid_columns]
                    total_cells = sum(cell_col_counts)
                    row_data = [rel_path, file_name, sheet_name, "Normal", emoji, category, total_cells] + cell_col_counts
                    data_rows_cells.append(row_data)

            processed_files += 1
            print(t('UI_013', current_language).format(f"{processed_files}/{len(files_to_process)}"))

        except Exception as e:
            print(f"{t('UI_017', current_language)}: {file_name} {t('UI_018', current_language)}: {e}")
            continue

    print(f"\n{t('UI_014', current_language)}")
    
    # ì‹œíŠ¸ì— ë°ì´í„° ì¶”ê°€
    sorted_columns = sorted(all_columns)
    column_headers = [f"Col {get_column_letter(col+1)}" for col in sorted_columns]
    headers = ['Path', 'FileName', 'SheetName', 'Status', 'ğŸ³ï¸', 'Category', 'TotalWords'] + column_headers
    
    # Words_real ì‹œíŠ¸ì— ë°ì´í„° ì¶”ê°€
    report_ws_real.append(headers)
    for row in data_rows_real:
        report_ws_real.append(row)
    adjust_column_widths(report_ws_real)

    # Words_unique_for_Sheet ì‹œíŠ¸ì— ë°ì´í„° ì¶”ê°€
    report_ws_unique_for_sheet.append(headers)
    for row in data_rows_unique_for_sheet:
        report_ws_unique_for_sheet.append(row)
    adjust_column_widths(report_ws_unique_for_sheet)
    
    # Words_unique_for_Folder ì‹œíŠ¸ì— ë°ì´í„° ì¶”ê°€
    report_ws_unique_for_folder.append(headers)
    for category in sorted(all_categories):
        if category in ['html_xml', 'brackets', 'newlines', 'file_paths']:
            emoji = 'ğŸ”§'
        else:
            emoji = 'ğŸŒ'
        
        unique_words = temp_manager.get_all_unique_words(category)
        total_unique_words = len(unique_words)
        row_data = ['ALL', 'ALL', 'ALL', 'Normal', emoji, category, total_unique_words] + [0] * len(sorted_columns)
        report_ws_unique_for_folder.append(row_data)
    adjust_column_widths(report_ws_unique_for_folder)
    
    # Words_cell_address ì‹œíŠ¸ì— ë°ì´í„° ì¶”ê°€
    cell_address_headers = headers.copy()
    cell_address_headers[5] = 'TotalCells'  # F1 ì…€ì˜ í—¤ë” ë³€ê²½
    report_ws_cell_address.append(cell_address_headers)
    for row in data_rows_cell_address:
        report_ws_cell_address.append(row)
    adjust_column_widths(report_ws_cell_address)
    
    # Words_cells ì‹œíŠ¸ì— ë°ì´í„° ì¶”ê°€
    cells_headers = headers.copy()
    cells_headers[5] = 'TotalCells'  # F1 ì…€ì˜ í—¤ë” ë³€ê²½
    report_ws_cells.append(cells_headers)
    for row in data_rows_cells:
        report_ws_cells.append(row)
    adjust_column_widths(report_ws_cells)

    # ì„ì‹œ íŒŒì¼ ì •ë¦¬
    temp_manager.cleanup()

    report_wb.save(report_path)
    print(f"{t('UI_015', current_language)}: {report_path}")

if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        print(f"Error: {e}")
        input("Press any key to continue...")
